{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNWSkgjpgltq9OpTUd/k2jj"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "193ef86c9fdb4a06a260ef75668516e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff0131c8b10a4143bd121e81cd4d1e91",
       "IPY_MODEL_065406b3251945a3928622a4c3e81e99",
       "IPY_MODEL_a0ef6e076f1d4973a79d3475af086ed4"
      ],
      "layout": "IPY_MODEL_92b33b2e03e84c439ca76e91c11d2368"
     }
    },
    "ff0131c8b10a4143bd121e81cd4d1e91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_178d99555b9b488a9f2338baa78b0637",
      "placeholder": "​",
      "style": "IPY_MODEL_1910cba8a332433b8f42d0709cebd39f",
      "value": "Map: 100%"
     }
    },
    "065406b3251945a3928622a4c3e81e99": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a41457b0d1f44505b0f9f1b183182d4f",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f1612b95c5f4a1c8c7b06596e76b5a9",
      "value": 29
     }
    },
    "a0ef6e076f1d4973a79d3475af086ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2399e89a0c11471389fd31144324be93",
      "placeholder": "​",
      "style": "IPY_MODEL_ce328bbb83b7463c89ce48d5442b16c2",
      "value": " 29/29 [00:00&lt;00:00, 1641.81 examples/s]"
     }
    },
    "92b33b2e03e84c439ca76e91c11d2368": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "178d99555b9b488a9f2338baa78b0637": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1910cba8a332433b8f42d0709cebd39f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a41457b0d1f44505b0f9f1b183182d4f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f1612b95c5f4a1c8c7b06596e76b5a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2399e89a0c11471389fd31144324be93": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce328bbb83b7463c89ce48d5442b16c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc14ac9e1614438597049719138d9e07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed950231ba774f9e8b3ada9a4421ff81",
       "IPY_MODEL_879ff1e12b684dae9858f3a01cc39ed4",
       "IPY_MODEL_d80b84e42c52456ba7914ff739719e04"
      ],
      "layout": "IPY_MODEL_7a04be94134e4661824ca4e92e61ecbe"
     }
    },
    "ed950231ba774f9e8b3ada9a4421ff81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_092a3ba52bbc426894c8bedfeeaabded",
      "placeholder": "​",
      "style": "IPY_MODEL_935dd93b08ff40da827c32990e87be7f",
      "value": "Map (num_proc=2): 100%"
     }
    },
    "879ff1e12b684dae9858f3a01cc39ed4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3ab2034cc6a74ff5a4bcf5fd4c57b2fa",
      "max": 29,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b906ca7d00842d886b2a7ac0b413cdd",
      "value": 29
     }
    },
    "d80b84e42c52456ba7914ff739719e04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3efcc644267b4b62ae4f5d032e25f7c7",
      "placeholder": "​",
      "style": "IPY_MODEL_0f3b7ba5fc8a4acd9d7a7c1571b2931b",
      "value": " 29/29 [00:00&lt;00:00, 60.09 examples/s]"
     }
    },
    "7a04be94134e4661824ca4e92e61ecbe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "092a3ba52bbc426894c8bedfeeaabded": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "935dd93b08ff40da827c32990e87be7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ab2034cc6a74ff5a4bcf5fd4c57b2fa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b906ca7d00842d886b2a7ac0b413cdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3efcc644267b4b62ae4f5d032e25f7c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f3b7ba5fc8a4acd9d7a7c1571b2931b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton\n",
    "import torch\n",
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-11T02:58:33.387585Z",
     "start_time": "2024-09-11T02:58:27.587105Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "#     \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "#     \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "#     \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "#     \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "#     \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "#     \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "#     \"unsloth/mistral-7b-bnb-4bit\",\n",
    "#     \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2-7B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y39ue1Xxz1bL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962084447,
     "user_tz": -480,
     "elapsed": 18089,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "4c20f101-ca79-4f6a-a797-a037e59b0d07",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:01:23.142339Z",
     "start_time": "2024-09-11T02:58:45.711145Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4070 SUPER. Max memory: 11.994 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/5.55G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b92b24765c7b437892d92e43c2d2d31a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a85e643b16ae4453b712f95127b8175f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5af24a6a89b94320b179b71d1eb01db9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab6a91ca4c8142c5bf68c2122216da56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3b0f09c04cc49eca9e1d5b08c4a2e62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d5329c4f783c48e5ac54aa467b62f1f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/367 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2568a3b77824e978eac2287bb6ce4cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e05389e462f24b2c8fca533e53cdb341"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1111"
   ],
   "metadata": {
    "id": "SYgxXGKmFXdF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725962102106,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     },
     "user_tz": -480
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "ad47bcf6-6fec-40ab-eee8-62814bdbd595",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:08:34.994997Z",
     "start_time": "2024-09-11T03:08:33.098481Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 28 layers with 0 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('json', data_files='/train.jsonl', split='train')\n",
    "print(dataset.column_names)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUW0IK-L0reg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962112370,
     "user_tz": -480,
     "elapsed": 813,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "ee7115ee-d645-4102-acb6-599b71d5fd85",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:09:31.972400Z",
     "start_time": "2024-09-11T03:09:30.392550Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8337e0c7b6c8441f9b605be174c7cbf1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instruction', 'input', 'output']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import to_sharegpt\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
    "    output_column_name = \"output\",\n",
    "    conversation_extension = 3, # Select more to handle longer conversations\n",
    ")"
   ],
   "metadata": {
    "id": "Fe9RHTRj3UfW",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962113024,
     "user_tz": -480,
     "elapsed": 16,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-11T03:09:37.510360Z",
     "start_time": "2024-09-11T03:09:35.819312Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Merging columns:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "825ea803bff7424abecc101708b662a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Converting to ShareGPT:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c432d056ed948e298d60784bfa10248"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad1f48cd21514bb99211db42707f3ec4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e175e4e9b9964a3fb964d03a8dab8e42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Flattening the indices:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "280f1a9c3eea4b9a847c93d9e1ac8bc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extending conversations:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c755bcc56ff4fd088d906cb625f15bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from unsloth import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)"
   ],
   "metadata": {
    "id": "ndsRASy03aIO",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962117073,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-09-11T03:09:42.943533Z",
     "start_time": "2024-09-11T03:09:42.530349Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Standardizing format:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6de5feecbf6f4f3c84797f841f4d8c74"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgpAOQEP3jFm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962118405,
     "user_tz": -480,
     "elapsed": 4,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "f56bbb41-52fe-458c-8b4b-550c6e41afac",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:10:22.774499Z",
     "start_time": "2024-09-11T03:10:22.727364Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'content': '中企动力和中企：\\nYour input is:\\n中企和中企动力是一家公司吗？', 'role': 'user'}, {'content': '‘中企动力”简称‘中企”，是一家成立于1999年的老牌互联网公司', 'role': 'assistant'}, {'content': '中企可以为客户制作以下门户网站：\\nYour input is:\\n中企能做什么类型的门户网站？', 'role': 'user'}, {'content': '内贸营销，全球营销，通用版本', 'role': 'assistant'}, {'content': '中企Saas商城有以下合作客户：\\nYour input is:\\n中企商城产品有哪些服务过的客户？', 'role': 'user'}, {'content': '湖南乐秀商贸有限公司、扬州老扬城食品有限公司、海门市创佳纺织品有限公司、珠海市斗门永生隆贸易有限公司、惠州市长布科技农业、扬州兴扬农产品...', 'role': 'assistant'}]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "chat_template = \"\"\"{SYSTEM}\n",
    "USER: {INPUT}\n",
    "ASSISTANT: {OUTPUT}\"\"\"\n",
    "from unsloth import apply_chat_template\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    chat_template = chat_template,\n",
    "    default_system_message = \"你是中企动力的客服\",\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "193ef86c9fdb4a06a260ef75668516e4",
      "ff0131c8b10a4143bd121e81cd4d1e91",
      "065406b3251945a3928622a4c3e81e99",
      "a0ef6e076f1d4973a79d3475af086ed4",
      "92b33b2e03e84c439ca76e91c11d2368",
      "178d99555b9b488a9f2338baa78b0637",
      "1910cba8a332433b8f42d0709cebd39f",
      "a41457b0d1f44505b0f9f1b183182d4f",
      "3f1612b95c5f4a1c8c7b06596e76b5a9",
      "2399e89a0c11471389fd31144324be93",
      "ce328bbb83b7463c89ce48d5442b16c2"
     ]
    },
    "id": "DKoU1Vbb3zYh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962120534,
     "user_tz": -480,
     "elapsed": 78,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "4b522603-a97a-4f38-d2fd-9a7c60184756",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:11:12.878231Z",
     "start_time": "2024-09-11T03:11:12.524804Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cee9aafa6a7e4db2ae84a3fe71f13c14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(dataset[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DsGCdU04OPhb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962124396,
     "user_tz": -480,
     "elapsed": 7,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "235ab788-72bf-4138-d554-063a056badb6",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:11:20.416270Z",
     "start_time": "2024-09-11T03:11:20.240524Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'content': '中企动力和中企：\\nYour input is:\\n中企和中企动力是一家公司吗？', 'role': 'user'}, {'content': '‘中企动力”简称‘中企”，是一家成立于1999年的老牌互联网公司', 'role': 'assistant'}, {'content': '中企可以为客户制作以下门户网站：\\nYour input is:\\n中企能做什么类型的门户网站？', 'role': 'user'}, {'content': '内贸营销，全球营销，通用版本', 'role': 'assistant'}, {'content': '中企Saas商城有以下合作客户：\\nYour input is:\\n中企商城产品有哪些服务过的客户？', 'role': 'user'}, {'content': '湖南乐秀商贸有限公司、扬州老扬城食品有限公司、海门市创佳纺织品有限公司、珠海市斗门永生隆贸易有限公司、惠州市长布科技农业、扬州兴扬农产品...', 'role': 'assistant'}], 'text': '你是中企动力的客服\\nUSER: 中企动力和中企：\\nYour input is:\\n中企和中企动力是一家公司吗？\\nASSISTANT: ‘中企动力”简称‘中企”，是一家成立于1999年的老牌互联网公司<|im_end|>\\nUSER: 中企可以为客户制作以下门户网站：\\nYour input is:\\n中企能做什么类型的门户网站？\\nASSISTANT: 内贸营销，全球营销，通用版本<|im_end|>\\nUSER: 中企Saas商城有以下合作客户：\\nYour input is:\\n中企商城产品有哪些服务过的客户？\\nASSISTANT: 湖南乐秀商贸有限公司、扬州老扬城食品有限公司、海门市创佳纺织品有限公司、珠海市斗门永生隆贸易有限公司、惠州市长布科技农业、扬州兴扬农产品...<|im_end|>'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        # num_train_epochs = 1, # For longer training runs!\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "fc14ac9e1614438597049719138d9e07",
      "ed950231ba774f9e8b3ada9a4421ff81",
      "879ff1e12b684dae9858f3a01cc39ed4",
      "d80b84e42c52456ba7914ff739719e04",
      "7a04be94134e4661824ca4e92e61ecbe",
      "092a3ba52bbc426894c8bedfeeaabded",
      "935dd93b08ff40da827c32990e87be7f",
      "3ab2034cc6a74ff5a4bcf5fd4c57b2fa",
      "9b906ca7d00842d886b2a7ac0b413cdd",
      "3efcc644267b4b62ae4f5d032e25f7c7",
      "0f3b7ba5fc8a4acd9d7a7c1571b2931b"
     ]
    },
    "id": "Vff_ahwX37oD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962126901,
     "user_tz": -480,
     "elapsed": 514,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "1c2615c1-0269-48cf-aa85-67e76c54c0cf",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:11:37.050975Z",
     "start_time": "2024-09-11T03:11:36.221958Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=2):   0%|          | 0/29 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91ae4e31a97b449eafd7decc9edd2d03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j10fSyiR4EJD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962131145,
     "user_tz": -480,
     "elapsed": 6,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "21fac591-576c-4b05-df47-382febcc846f",
    "ExecuteTime": {
     "end_time": "2024-09-11T03:11:44.916391Z",
     "start_time": "2024-09-11T03:11:44.857673Z"
    }
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4070 SUPER. Max memory = 11.994 GB.\n",
      "5.764 GB of memory reserved.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "trainer_stats = trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "shorM3sB4HGq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962310730,
     "user_tz": -480,
     "elapsed": 176913,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "d22d17f9-bc64-4fd5-bf8e-1328cb4bfc76",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-09-11T03:11:50.294735Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 29 | Num Epochs = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/60 : < :, Epoch 0.27/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olL0ZWrq4X4A",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962356694,
     "user_tz": -480,
     "elapsed": 3,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "cba7ccc1-efc7-46cb-88eb-ce7ff9bcdd41",
    "ExecuteTime": {
     "end_time": "2024-09-11T02:40:30.378343Z",
     "start_time": "2024-09-11T02:40:30.275471Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174.0767 seconds used for training.\n",
      "2.9 minutes used for training.\n",
      "Peak reserved memory = 8.041 GB.\n",
      "Peak reserved memory for training = 2.277 GB.\n",
      "Peak reserved memory % of max memory = 67.042 %.\n",
      "Peak reserved memory for training % of max memory = 18.984 %.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "messages = [                         # Change below!\n",
    "    {\"role\": \"user\",      \"content\": \"Saas商城能做什么\"}]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHXHxLci4Zvg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962365397,
     "user_tz": -480,
     "elapsed": 3398,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "b4eca453-8bf1-4863-bdea-e0b8edab91ce"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "为中小企业打造全渠道交易场景：\n",
      "1、满足企业品牌塑造和在线交易的官网独立商城，支持PC+H5双端呈现;\n",
      "2、全面支持企业ToG、ToB、ToC三大经营诉求，具备政采对接能力，包\n",
      "括央采、京华云采、各地省采等;<|im_end|>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"lora_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "#model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4vE-jBN41oQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962376866,
     "user_tz": -480,
     "elapsed": 1521,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "b6530b91-1eaa-4d2f-82a2-d6297bf4fbac",
    "ExecuteTime": {
     "end_time": "2024-09-11T02:40:36.311595Z",
     "start_time": "2024-09-11T02:40:34.926243Z"
    }
   },
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "('lora_model/tokenizer_config.json',\n 'lora_model/special_tokens_map.json',\n 'lora_model/vocab.json',\n 'lora_model/merges.txt',\n 'lora_model/added_tokens.json',\n 'lora_model/tokenizer.json')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PL3dG1X76Rkr",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962379503,
     "user_tz": -480,
     "elapsed": 3,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "e492a94b-4815-42a8-82ef-c57c1bbab205",
    "ExecuteTime": {
     "end_time": "2024-09-11T02:40:45.401179Z",
     "start_time": "2024-09-11T02:40:45.257879Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ],
   "metadata": {
    "id": "BD9u0JlzD5Pv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725955444097,
     "user_tz": -480,
     "elapsed": 40249,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "7c987dcd-9c8e-49c7-bb64-2607c24e3354",
    "ExecuteTime": {
     "end_time": "2024-09-11T02:55:27.290064Z",
     "start_time": "2024-09-11T02:54:46.302021Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\r\n",
      ">>> Downloading Linux amd64 bundle\r\n",
      "######################################################################## 100.0%\r#O#-#                                                                         #-#O=#  #                                                                        3.8% 27.0%              34.3% 46.7%#########                   76.7%########################       93.9%\r\n",
      ">>> Nvidia GPU detected.\r\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\r\n",
      ">>> Install complete. Run \"ollama\" from the command line.\r\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\r\n",
      ">>> Install complete. Run \"ollama\" from the command line.\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Save to 8bit Q8_0\n",
    "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "#if False:\n",
    "#    model.push_to_hub_gguf(\n",
    "#        \"hf/model\", # Change hf to your username!\n",
    "#        tokenizer,\n",
    "#        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "#        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "#    )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2O7AOSa9iRV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962484404,
     "user_tz": -480,
     "elapsed": 101176,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "6215a5c3-1a89-4d70-849c-61a014b8a394"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
      "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
      "Unsloth: Will remove a cached repo with size 5.5G\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 4.54 out of 15.53 RAM for saving.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 25%|██▌       | 7/28 [00:00<00:01, 11.53it/s]We will save to Disk and not RAM now.\n",
      "100%|██████████| 28/28 [00:11<00:00,  2.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Unsloth: Converting qwen2 model. Can use fast conversion = False.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q8_0'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at model into q8_0 GGUF format.\n",
      "The output location will be ./model/unsloth.Q8_0.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: model\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> Q8_0, shape = {3584, 152064}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.bias,         torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.bias,         torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {18944, 3584}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 18944}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.bias,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 3584}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.bias,        torch.bfloat16 --> F32, shape = {512}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {3584, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3584}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:output.weight,             torch.bfloat16 --> Q8_0, shape = {3584, 152064}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 3584\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 18944\n",
      "INFO:hf-to-gguf:gguf: head count = 28\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 4\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151645\n",
      "INFO:gguf.vocab:Setting special token type pad to 151643\n",
      "INFO:gguf.vocab:Setting special token type bos to 151643\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] }}{% set loop_messages = messages[1:] %}{% else %}{{ '你是中企动力的客服' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '\n",
      "USER: ' + message['content'] }}{% elif message['role'] == 'assistant' %}{{ '\n",
      "ASSISTANT: ' + message['content'] + '<|im_end|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n",
      "ASSISTANT: ' }}{% endif %}\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:model/unsloth.Q8_0.gguf: n_tensors = 339, total_size = 8.1G\n",
      "Writing:   0%|          | 0.00/8.09G [00:00<?, ?byte/s]/content/llama.cpp/gguf-py/gguf/quants.py:387: RuntimeWarning: overflow encountered in divide\n",
      "  id = np.where(d == 0, 0, 1 / d)\n",
      "/content/llama.cpp/gguf-py/gguf/quants.py:46: RuntimeWarning: invalid value encountered in subtract\n",
      "  b = floored + np.floor(2 * (a - floored))\n",
      "/content/llama.cpp/gguf-py/gguf/quants.py:393: RuntimeWarning: invalid value encountered in cast\n",
      "  qs = qs.astype(np.int8).view(np.uint8)\n",
      "/content/llama.cpp/gguf-py/gguf/quants.py:388: RuntimeWarning: invalid value encountered in multiply\n",
      "  qs = np_roundf(blocks * id)\n",
      "Writing: 100%|██████████| 8.09G/8.09G [01:06<00:00, 123Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to model/unsloth.Q8_0.gguf\n",
      "Unsloth: Conversion completed! Output location: ./model/unsloth.Q8_0.gguf\n",
      "Unsloth: Saved Ollama Modelfile to model/Modelfile\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "time.sleep(3) # Wait for a few seconds for Ollama to load!"
   ],
   "metadata": {
    "id": "yF32b1y_osZe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962502540,
     "user_tz": -480,
     "elapsed": 3006,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(tokenizer._ollama_modelfile)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esykClrum-Q3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962502592,
     "user_tz": -480,
     "elapsed": 51,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "36ade051-7a83-49eb-cad5-f2754c677d08"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FROM {__FILE_LOCATION__}\n",
      "\n",
      "TEMPLATE \"\"\"{{ if .System }}{{ .System }}{{ end }}{{ if .Prompt }}\n",
      "USER: {{ .Prompt }}{{ end }}\n",
      "ASSISTANT: {{ .Response }}<|im_end|>\"\"\"\n",
      "\n",
      "PARAMETER stop \"<|endoftext|>\"\n",
      "PARAMETER stop \"<|im_start|>\"\n",
      "PARAMETER stop \"<|im_end|>\"\n",
      "SYSTEM \"你是中企动力的客服\"\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ollama create tongyiqianwen -f ./model/Modelfile"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5QZ4Aeho1qu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725962529900,
     "user_tz": -480,
     "elapsed": 11897,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "bfed10e4-5c88-4085-a13e-d64a0cf514ac"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[?25ltransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠇ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠹ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠴ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠏ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠋ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠙ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠸ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠼ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠦ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data ⠧ \u001B[?25h\u001B[?25l\u001B[2K\u001B[1Gtransferring model data \n",
      "creating new layer sha256:5b9751394baf6d5df5027f504b4cfff11b7a747a96ed22c038d01dba9e014271 \n",
      "creating new layer sha256:774728a249cf5366a622b616be2b87e2b09633cc7e95a944e5d7e44f34dfa87f \n",
      "creating new layer sha256:5b8fa1ff6c1520b5e4b1fd4c50d2c317b90c2ce9b92b2df00668eccf98c337bb \n",
      "creating new layer sha256:e61a8a8f0518046f53463d2ed746e0e70ccea7bf1657c61118b6991240b42fc7 \n",
      "creating new layer sha256:9f091c4fd652b54361293bca21948e95a226fd28fce36daba3f1ee233c43de23 \n",
      "writing manifest \n",
      "success \u001B[?25h\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "ls /content/model"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d72Te2ijus7E",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1725951668917,
     "user_tz": -480,
     "elapsed": 105,
     "user": {
      "displayName": "桑桑",
      "userId": "15939856000553134307"
     }
    },
    "outputId": "3f7f47ca-cc55-4b27-9c69-d97dc30f44fe"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "config.json                       Modelfile                     unsloth.F16.gguf\n",
      "generation_config.json            model.safetensors.index.json  unsloth.Q4_K_M.gguf\n",
      "model-00001-of-00004.safetensors  special_tokens_map.json       unsloth.Q5_K_M.gguf\n",
      "model-00002-of-00004.safetensors  tokenizer_config.json         unsloth.Q8_0.gguf\n",
      "model-00003-of-00004.safetensors  tokenizer.json\n",
      "model-00004-of-00004.safetensors  unsloth.BF16.gguf\n"
     ]
    }
   ]
  }
 ]
}
